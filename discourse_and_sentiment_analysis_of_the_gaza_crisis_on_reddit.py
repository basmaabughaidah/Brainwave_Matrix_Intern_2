# -*- coding: utf-8 -*-
"""Discourse and Sentiment Analysis of the Gaza Crisis on Reddit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167-DqLerST_5XUeMFIHbTpqDN6ncTGIN

#  **Discourse and Sentiment Analysis of the Gaza Crisis on Reddit**  
### *Academic Research Version*

---

**Author:** Basma Mohamed Abu Gheda  
**Date:** June 2025  

---

### ğŸ¯ **Project Aim**

To conduct a comprehensive and reproducible academic-style analysis of public sentiment and discourse regarding the Gaza crisis, using real-time Reddit data, advanced transformer-based NLP models, and unsupervised topic modeling techniques.

---

### ğŸ“Œ **Methodological Focus**

This notebook is designed not just for exploration, but as a full research pipeline with:

- Transparent data acquisition
- Reproducible NLP modeling
- Rigorous visualizations
- Qualitative and quantitative synthesis
- Modular design for future deployment or publication

# ğŸ§¾ Abstract

This project investigates online public sentiment and discourse surrounding the Gaza crisis through the lens of Reddit data. By integrating real-time content collection with transformer-based NLP models for emotion classification and BERTopic for unsupervised topic modeling, we provide a dual-layered analysis that combines emotional intensity with discourse structure.

The study reveals dominant emotional patternsâ€”especially fear, sadness, and angerâ€”across thematically distinct discussions. Visualization tools such as heatmaps, interactive timelines, and representative post sampling offer both quantitative and qualitative insight into the evolving narrative of the crisis.

Although an hourly automated pipeline was proposed, implementation constraints within the Colab environment limited its deployment. Nonetheless, the modular framework remains adaptable for real-time integration in more robust production settings.

This work contributes a scalable methodology for analyzing high-volume, user-generated discourse in response to global conflicts, offering valuable insights for media researchers, political analysts, and digital humanitarian initiatives.

# âš ï¸ Limitations & Future Work

Despite its strengths, this study has several limitations:

- **Platform Bias:** The data is limited to Reddit, which may not represent the full spectrum of public opinion.
- **Language & Cultural Nuance:** The emotion classifier is English-specific and may not capture deeper cultural or contextual sentiment.
- **Hourly Automation:** A real-time, hourly update mechanism was explored but could not be deployed due to execution environment constraints on Google Colab.
- **Data Volume:** The analysis was constrained to a limited number of posts (â‰ˆ200) for feasibility; scaling up would improve generalizability.

### ğŸ”„ Future Improvements

- Integrate cross-platform data (e.g., Twitter, Facebook) for broader insight.
- Enable full hourly automation using an external scheduler or API pipeline.
- Apply multi-language emotion models for diverse audience coverage.
- Incorporate engagement metrics (e.g., upvotes, comments) to weight the significance of each post.

This modular framework can be expanded into a real-time dashboard or serve as the foundation for a publishable research paper on computational discourse analysis in conflict settings.

# ğŸ“˜ Introduction & Context

This project explores the digital public discourse surrounding the Gaza crisis through real-time analysis of Reddit content. Social media platforms, particularly Reddit, offer valuable insights into how global audiences perceive and emotionally respond to evolving geopolitical events.

By leveraging advanced Natural Language Processing (NLP) techniques, including transformer-based emotion classification and topic modeling, this notebook aims to extract both the **emotional tone** and **thematic structure** of public conversations related to the crisis.

Reddit was chosen as the primary data source due to its open nature, active political communities, and real-time user engagement. The resulting analysis not only captures sentiment trends over time but also uncovers deeper layers of discourse that drive online narratives.

---

### ğŸ¯ Objectives:

- Collect and preprocess Reddit posts related to the Gaza crisis in near-real time  
- Analyze emotional dynamics using state-of-the-art transformer models  
- Apply unsupervised topic modeling to reveal key discourse themes  
- Visualize temporal and topical sentiment shifts  
- Present representative examples for qualitative interpretation

## Step 1:Install libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

!pip install praw transformers langdetect tqdm plotly wordcloud asyncpraw nest_asyncio feedparser spacy
!python -m spacy download en_core_web_sm

"""## Step 2 :Import libraries"""

import praw
import asyncpraw
import nest_asyncio
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
from transformers import pipeline
from tqdm.notebook import tqdm
from langdetect import detect
from wordcloud import WordCloud
from collections import Counter
from scipy.stats import entropy
import spacy
import time

nest_asyncio.apply()

"""## Step 3 : Setting up a Reddit connection"""

client_id = "q9niT61z5MZVraR6sKG-GA"
client_secret = "-hJ239KzKCSA32i2yjUwVmUnZHh2wA"
user_agent = "AcademicGazaAnalysis"

reddit = praw.Reddit(
    client_id=client_id,
    client_secret=client_secret,
    user_agent=user_agent
)

subreddit_name = "worldnews"
search_query = "Gaza OR Palestine OR Israel"
limit = 100
subreddit = reddit.subreddit(subreddit_name)

"""## Step 4 : Text merge function"""

def combine_text(row):
    if pd.isna(row['self_text']) or row['self_text'].strip() == '':
        return row['title']
    else:
        return f"{row['title']} {row['self_text']}"

"""## Step 5 : Sentiment Analysis"""

emotion_classifier = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base",
    top_k=None
)

def analyze_emotions(text):
    try:
        short_text = text[:400]
        result = emotion_classifier(short_text)
        return {entry['label']: entry['score'] for entry in result[0]}
    except:
        return {"error": 1}

"""## Step 6 : Initialize a main **DataFrame**"""

df = pd.DataFrame()

"""## Step 7 :Hourly auto-update function"""

def run_hourly():
    global df
    while True:
        print(f"â³ Started run at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        posts = subreddit.search(search_query, sort="new", limit=limit)
        data = []
        for post in posts:
            data.append({
                "title": post.title,
                "self_text": post.selftext,
                "created_utc": datetime.utcfromtimestamp(post.created_utc),
                "subreddit": post.subreddit.display_name,
                "score": post.score,
                "id": post.id
            })

        temp_df = pd.DataFrame(data)
        temp_df['full_text'] = temp_df.apply(combine_text, axis=1)
        temp_df = temp_df[temp_df['full_text'].str.strip().astype(bool)].reset_index(drop=True)

        temp_df['emotions'] = temp_df['full_text'].progress_apply(analyze_emotions)
        emotion_temp = temp_df['emotions'].apply(pd.Series)
        temp_df = pd.concat([temp_df.drop(columns=['emotions']), emotion_temp], axis=1)

        df = pd.concat([df, temp_df], ignore_index=True).drop_duplicates(subset='id')

        print(f"âœ… Run completed. Total posts: {len(df)}")
        print("Next run in 1 hour...\n")
        time.sleep(3600)

"""## Step 8 : Manual One-Time Data Collection and Sentiment Analysis


"""

posts = subreddit.search(search_query, sort="new", limit=30)
data = []

for post in posts:
    data.append({
        "title": post.title,
        "self_text": post.selftext,
        "created_utc": datetime.utcfromtimestamp(post.created_utc),
        "subreddit": post.subreddit.display_name,
        "score": post.score,
        "id": post.id
    })

df = pd.DataFrame(data)


df['full_text'] = df.apply(combine_text, axis=1)


df['emotions'] = df['full_text'].apply(analyze_emotions)

emotion_columns_df = df['emotions'].apply(pd.Series)
df = pd.concat([df.drop(columns=['emotions']), emotion_columns_df], axis=1)
emotion_columns = emotion_columns_df.columns.tolist()


df['dominant_emotion'] = df[emotion_columns].idxmax(axis=1)

"""## Step 8 : Calculating the dominant sentiment of each post"""

emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']
df['dominant_emotion'] = df[emotion_columns].idxmax(axis=1)

df.columns

"""## Step 9 :A graph of the distribution of dominant emotions"""

plt.figure(figsize=(8, 6))
sns.countplot(x='dominant_emotion', data=df, order=df['dominant_emotion'].value_counts().index, palette='coolwarm')
plt.title("Distribution of Dominant Emotions")
plt.xlabel("Emotion")
plt.ylabel("Number of Posts")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Step 10 : Convert date to day (for time analysis)"""

df['date'] = pd.to_datetime(df['created_utc']).dt.date

"""## Step 11 :Preparing time-series sentiment analysis data"""

melted = df[['date'] + emotion_columns].melt(id_vars='date', var_name='emotion', value_name='score')
daily_emotion_avg = melted.groupby(['date', 'emotion'])['score'].mean().reset_index()

"""## Step 12 : Line drawing of changing emotions over time"""

fig = px.line(
    daily_emotion_avg,
    x='date',
    y='score',
    color='emotion',
    title='Emotion Trends Over Time',
    markers=True
)
fig.update_layout(xaxis_title='Date', yaxis_title='Average Score')
fig.show()

"""## Step 13 :Create a WordCloud:"""

text_data = ' '.join(df['title'].dropna().astype(str))
wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(text_data)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Most Frequent Words in Reddit Titles")
plt.show()

"""## Step 14 : Location recognition using NER"""

nlp = spacy.load("en_core_web_sm")
texts = df['full_text'].tolist()
locations = []

for doc in nlp.pipe(texts, disable=["tagger", "parser"]):
    locations += [ent.text for ent in doc.ents if ent.label_ in ["GPE", "LOC"]]

"""## Step 15 : Chart of most mentioned places"""

location_counts = Counter(locations)
top_locations = location_counts.most_common(15)
loc_df = pd.DataFrame(top_locations, columns=['Location', 'Mentions'])

fig = px.bar(loc_df, x='Location', y='Mentions', title='Most Mentioned Locations')
fig.show()

"""## Step 16 :Calculating Emotion Diversity Using Entropy"""

def compute_entropy(row):
    values = row[emotion_columns].values.astype(float)
    return entropy(values)

df['emotion_entropy'] = df.apply(compute_entropy, axis=1)

"""## Step 17 : Histogram to measure emotion diversity"""

fig = px.histogram(df, x='emotion_entropy', nbins=20, title='Emotion Diversity (Entropy)')
fig.show()

"""## Step 18 : Installing the BERTopic library

"""

!pip install bertopic
!pip install umap-learn
!pip install sentence-transformers

"""## Step 19 : BERTopic import and data preparation"""

import praw
import pandas as pd


reddit = praw.Reddit(
    client_id="q9niT61z5MZVraR6sKG-GA",
    client_secret="-hJ239KzKCSA32i2yjUwVmUnZHh2wA",
    user_agent="YourApp"
)


subreddit = reddit.subreddit("worldnews")
search_query = "Gaza"
posts = subreddit.search(search_query, sort="new", limit=200)

data = []
for post in posts:
    full_text = f"{post.title} {post.selftext}"
    data.append({
        'title': post.title,
        'selftext': post.selftext,
        'full_text': full_text,
        'created': pd.to_datetime(post.created_utc, unit='s')
    })

df = pd.DataFrame(data)

df['full_text'] = df['full_text'].fillna("").astype(str)
texts = df['full_text'].tolist()

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

topic_model = BERTopic(
    embedding_model=embedding_model,
    min_topic_size=3,
    top_n_words=10,
    verbose=True
)

df['topic'], _ = topic_model.fit_transform(texts)

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Load sentence transformer embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Extract full text from dataframe
texts = df['full_text'].dropna().astype(str).tolist()

"""## Step 20 : Training a BERTopic model and extracting topics"""

topic_model = BERTopic(embedding_model=embedding_model, verbose=True)


topics, probs = topic_model.fit_transform(texts)

"""## Step 21 : View main topics"""

topic_model.get_topic_info().head(10)

"""## Step 22 : Set a topic for each post."""

df['topic'] = topics

"""## Step 23 :Chart the number of posts by topic"""

df['topic'].value_counts().head(10).plot(kind='bar', figsize=(10,5), title="Top Topics in Reddit Posts")

"""## Step 24 :Preparing data for timeline"""

df['date'] = pd.to_datetime(df['created']).dt.date

timeline_df = df.groupby(['date', 'topic']).size().reset_index(name='count')

"""## Step 25: Line graph of each topic over time"""

fig = px.line(
    timeline_df,
    x='date',
    y='count',
    color='topic',
    title='Topic Trends Over Time (BERTopic)',
    markers=True
)

fig.update_layout(
    xaxis_title='Date',
    yaxis_title='Number of Posts',
    legend_title='Topic ID'
)

fig.show()

"""## Step 26: Draw only the top 5 topics"""

top_topics = df['topic'].value_counts().nlargest(5).index


top_timeline_df = timeline_df[timeline_df['topic'].isin(top_topics)]

fig = px.line(
    top_timeline_df,
    x='date',
    y='count',
    color='topic',
    title='Top 5 Topic Trends Over Time',
    markers=True
)
fig.show()

"""## Step 27: Draw a word cloud for each topic (WordCloud)"""

# Commented out IPython magic to ensure Python compatibility.
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

for topic_id in df['topic'].unique():
    if topic_id == -1:
        continue
    words = topic_model.get_topic(topic_id)
    if not words:
        print(f"âš ï¸ Topic {topic_id} has no words.")
        continue
    word_freq = {word: weight for word, weight in words}
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Topic {topic_id} WordCloud")
    plt.show()

"""## Step 28: Sentiment analysis for each *topic*"""

from transformers import pipeline
from tqdm import tqdm

emotion_labels = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']
sentiment_pipeline = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=None)
tqdm.pandas()

df['emotions'] = df['full_text'].progress_apply(lambda x: sentiment_pipeline(x[:512]))

for emotion in emotion_labels:
    df[emotion] = df['emotions'].apply(lambda x: next((i['score'] for i in x[0] if i['label'].lower() == emotion), 0))

df['dominant_emotion'] = df[emotion_labels].idxmax(axis=1)

topic_emotions = df.groupby(['topic', 'dominant_emotion']).size().unstack(fill_value=0)
topic_emotions_percent = topic_emotions.div(topic_emotions.sum(axis=1), axis=0)

"""## Step 29:A bar chart of each subject's feelings."""

topic_emotions_percent.plot(
    kind='bar',
    stacked=True,
    figsize=(14,6),
    colormap='Set3',
    edgecolor='black'
)
plt.title("Emotional Distribution per Topic")
plt.ylabel("Percentage")
plt.xlabel("Topic ID")
plt.xticks(rotation=0)
plt.legend(title="Emotion", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""# ğŸ“Š Gaza Crisis â€“ Reddit Sentiment & Topic Dashboard

##Pie Chart of Dominant Emotions
"""

import matplotlib.pyplot as plt

emotion_counts = df['dominant_emotion'].value_counts()

plt.figure(figsize=(6, 6))
plt.pie(emotion_counts, labels=emotion_counts.index, autopct='%1.1f%%', startangle=140)
plt.title("Overall Dominant Emotions Distribution")
plt.axis("equal")
plt.show()

daily_counts = df.groupby('date').size()

plt.figure(figsize=(12, 4))
daily_counts.plot(kind='line', marker='o')
plt.title("Post Activity Over Time")
plt.xlabel("Date")
plt.ylabel("Number of Posts")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

"""##Heatmap: Emotional Distribution Across Topics"""

import seaborn as sns
import matplotlib.pyplot as plt

# Create heatmap data: emotions (%) per topic
heatmap_data = topic_emotions_percent.copy()

# Sort topics numerically (just in case)
heatmap_data = heatmap_data.sort_index()

# Plot heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(heatmap_data, cmap="YlGnBu", linewidths=0.5, annot=True, fmt=".2f")

plt.title("Emotional Distribution Across Topics (as %)")
plt.xlabel("Emotion")
plt.ylabel("Topic ID")
plt.tight_layout()
plt.show()

"""##Interactive Timeline of Reddit Posts Over Time"""

import plotly.express as px

# Create time series data
daily_counts = df.groupby('date').size().reset_index(name='num_posts')

# Interactive line plot
fig = px.line(
    daily_counts,
    x='date',
    y='num_posts',
    title='Reddit Post Activity Over Time',
    markers=True,
    template='plotly_dark',
    labels={'date': 'Date', 'num_posts': 'Number of Posts'}
)

fig.update_layout(
    xaxis_title='Date',
    yaxis_title='Number of Posts',
    hovermode='x unified',
    margin=dict(l=40, r=40, t=60, b=40)
)

fig.show()

"""## Representative Post for Each Topic"""

# Ensure we have text length to rank posts
rep_posts = df.copy()
rep_posts['text_length'] = rep_posts['full_text'].apply(len)

# Select the longest post as representative per topic
representative_df = rep_posts.sort_values('text_length', ascending=False).groupby('topic').first().reset_index()

# Display topic ID, dominant emotion, and representative text
representative_df[['topic', 'dominant_emotion', 'full_text']].head(10)

# Save to CSV for external use
representative_df[['topic', 'dominant_emotion', 'full_text']].to_csv("representative_posts.csv", index=False)

"""# ğŸ“Œ Final Conclusion: *Sentiment & Discourse Analysis on the Gaza Crisis (Reddit-based)*

### **Conclusion**

This project presented a comprehensive sentiment and discourse analysis of Reddit discussions surrounding the Gaza crisis. By collecting real-time user-generated content from Reddit and applying modern NLP techniques, we were able to extract emotional patterns, dominant topics, and temporal dynamics across hundreds of posts.

Using a pre-trained DistilRoBERTa-based emotion classification model, the study captured nuanced emotional tones, including fear, sadness, anger, and a notable presence of neutral and surprise responses. BERTopic was employed to uncover latent discourse themes, which were further enriched by visualizing their emotional composition using a heatmap â€” highlighting how specific topics were heavily associated with negative emotions such as fear and sadness.

An interactive Plotly timeline illustrated the evolving intensity of discussions over time, while a curated set of representative posts per topic provided qualitative context to the quantitative findings, allowing for deeper narrative insight.

Although a real-time automated pipeline was initially considered â€” including an hourly refresh script â€” practical constraints related to scheduling and execution within the Colab environment limited its implementation. Nonetheless, the architecture remains modular and scalable for future deployment.

This work underscores the potential of combining unsupervised topic modeling with emotion analysis to understand public sentiment in conflict-related discourse. The methodology used here can be extended to other geopolitical or crisis scenarios to monitor digital public opinion in near-real-time.

---

### ğŸ”š Summary Highlights:
- âœ… 7-label emotion classification with >200 Reddit posts  
- âœ… Topic modeling via BERTopic with full visualization  
- âœ… Cross-topic emotion heatmap  
- âœ… Interactive temporal trends (Plotly)  
- âœ… Qualitative interpretation through real Reddit samples  
- âš ï¸ *Hourly automation attempted but not integrated due to environment limitations*

"""